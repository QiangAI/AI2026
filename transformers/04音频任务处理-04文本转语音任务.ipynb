{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36debbf5-987e-42f4-8c57-2dc8a354fd18",
   "metadata": {},
   "source": [
    "# 4. 文本转语音任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6d663-9a6b-412c-981c-c9a65c21d769",
   "metadata": {},
   "source": [
    "- 文本转语音（Text-to-Speech，TTS）是指根据文本输入生成自然流畅语音的任务。TTS 模型可进一步扩展为单模型支持多说话人、多语言的语音生成系统。文本转语音也是语音合成。在Transformers中分成：\n",
    "    - Text-to-Speech\n",
    "    - Text-to-Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e367e-6420-4d25-8145-7265409dcf8e",
   "metadata": {},
   "source": [
    "## 4.1. 文本转语音任务应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f1bc7-1a58-49b3-a766-d5954ace3363",
   "metadata": {},
   "source": [
    "- 在语音合成模型中，我们选择支持Transformers、魔塔与第三方等技术框架的模型,在transformers框架中实现的算法有：\n",
    "    - Mimi\n",
    "    - MMS\n",
    "    - Moshi\n",
    "    - Seamless-M4T\n",
    "    - SeamlessM4T-v2\n",
    "    - Speech2Text\n",
    "    - Speech2Text2\n",
    "    - SpeechT5\n",
    "    - Wav2Vec2\n",
    "    - Wav2Vec2-BERT\n",
    "    - Wav2Vec2-Conformer\n",
    "    - Wav2Vec2Phoneme\n",
    "    - Whisper\n",
    "- 在选择模型的时候可以根据这些算法关键字过滤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2937a-3d98-40a4-8ff5-d1d98f8e3736",
   "metadata": {},
   "source": [
    "### (1) 模型下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ddcd8-9cf3-4a02-b86a-b283fcef92dc",
   "metadata": {},
   "source": [
    "- 微软（microsoft）模型\n",
    "    - `git clone https://www.modelscope.cn/microsoft/speecht5_tts.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2808cc-010a-467e-9a60-57d29f75bfd4",
   "metadata": {},
   "source": [
    "- facebook模型下载\n",
    "    - `git clone https://www.modelscope.cn/facebook/hf-seamless-m4t-large.git`\n",
    "    - `git clone https://www.modelscope.cn/facebook/musicgen-stereo-medium.git`(Text-to-Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97451917-c9eb-4151-a210-569ce4b8af29",
   "metadata": {},
   "source": [
    "- Transformers站点推荐的：\n",
    "    - `git clone https://www.modelscope.cn/ACE-Step/Ace-Step1.5.git`(Text-to-Audio)\n",
    "    - `git clone https://www.modelscope.cn/mapjack/bark.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bcf5f-6da1-4214-8c92-17cb836f3e32",
   "metadata": {},
   "source": [
    "### (2) 使用speecht5_tts模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a3da16-16e3-4236-8ec9-c64b4e84247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([ 0.00052379,  0.00074588,  0.00091089, ..., -0.00042187,\n",
      "       -0.00017906, -0.00015127], shape=(34816,), dtype=float32), 'sampling_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-to-speech\", model=\"F:/03Models/speecht5_tts\")\n",
    "\n",
    "embeddings_dataset = load_dataset(\"F:/04Datasets/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True)\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "speech = pipe(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "print(speech)\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825f2a6-93a0-464e-afa7-66748764c61e",
   "metadata": {},
   "source": [
    "### (3) 使用hf-seamless-m4t-large模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe9ffe1-8209-4b1a-8284-338c0ae0b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_to_audio.TextToAudioPipeline object at 0x000001BB29670190>\n",
      "{'audio': array([[-1.7293938e-04, -1.8664182e-04, -1.4298852e-04, ...,\n",
      "        -4.9232389e-05, -7.8199897e-05, -4.9895141e-05]],\n",
      "      shape=(1, 24640), dtype=float32), 'sampling_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "pipe = pipeline(\"text-to-speech\", model=\"F:/03Models/hf-seamless-m4t-large\")\n",
    "print(pipe)\n",
    "speech = pipe(\"我是中国人！\", forward_params={\"tgt_lang\": \"cmn\"})\n",
    "print(speech)\n",
    "# sf.write(\"speech_cmn.wav\", np.squeeze(speech[\"audio\"], axis=0), samplerate=speech[\"sampling_rate\"])\n",
    "print(speech[\"audio\"][0].shape)\n",
    "sf.write(\"speech_cmn.wav\", speech[\"audio\"][0], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819c59e-691d-44a6-926a-86dc9b31cd85",
   "metadata": {},
   "source": [
    "### (4) 使用bark模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0f442-aa71-429a-b860-873a111aa427",
   "metadata": {},
   "source": [
    "- Bark是一个由Suno创建的基于变换器的文本转音频模型。\n",
    "    - Bark能够生成高度逼真的多语言语音以及其他音频，包括音乐、背景噪音和简单的音效。\n",
    "    - 该模型还能产生非言语交流，如笑声、叹息和哭泣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e297aa7a-d00f-4524-bee4-0c03c3c5e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'F:/03Models/bark' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cuda:0\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型推理\n",
      "保存结果\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "print(\"加载模型\")\n",
    "synthesiser = pipeline(\"text-to-speech\", \"F:/03Models/bark\")\n",
    "print(\"模型推理\")\n",
    "speech = synthesiser(\"Hello, my dog  is cooler than you! [laughs]\", forward_params={\"do_sample\": True})\n",
    "print(\"保存结果\")\n",
    "print(speech[\"audio\"][0].shape)\n",
    "sf.write(\"bark_out.wav\", speech[\"audio\"][0], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4594729-66fd-4015-8e76-a501c7a80b46",
   "metadata": {},
   "source": [
    "- 代码说明：\n",
    "    - 注意输出模型的形状是(n, 2)格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c458bb-01e7-4bd2-a22a-20b2e3a71403",
   "metadata": {},
   "source": [
    "### (5) 使用musicgen-stereo-medium模型生成音乐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5b37f-8c49-4f63-a149-291249770e37",
   "metadata": {},
   "source": [
    "- 一组能够处理立体声的模型。\n",
    "    - 立体声音响，也称为立体声，是一种用于再现具有深度和方向感的声音的技术。它使用两个独立的音频通道通过扬声器（或耳机）播放，从而创造出声音来自多个方向的印象。\n",
    "    - MusicGen是一个文本到音乐的模型，能够根据文本描述或音频提示生成高质量的音乐样本。它是一个单一阶段的自回归Transformer模型，基于32kHz EnCodec分词器进行训练，该分词器有4个以50Hz采样的码本。与现有方法如MusicLM不同，MusicGen不需要自我监督的语义表示，并且可以在一次通过中生成所有4个码本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba520e41-09c1-4265-898a-d0682d460895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_to_audio.TextToAudioPipeline object at 0x00000262F0156710>\n",
      "{'audio': array([[[-0.04666138, -0.05020142, -0.04058838, ...,  0.06048584,\n",
      "          0.05670166,  0.05804443],\n",
      "        [-0.03741455, -0.04107666, -0.02902222, ...,  0.08093262,\n",
      "          0.07556152,  0.07885742]]], shape=(1, 2, 161920), dtype=float32), 'sampling_rate': 32000}\n",
      "(2, 161920)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "\n",
    "# 创建\n",
    "synthesiser = pipeline(\"text-to-audio\", \"F:/03Models/musicgen-stereo-medium\", device=\"cuda:0\", dtype=torch.float16)\n",
    "print(synthesiser)\n",
    "music = synthesiser(\"Chinese classical guzheng music\", forward_params={\"max_new_tokens\": 256})\n",
    "print(music)\n",
    "print(music[\"audio\"][0].shape)\n",
    "sf.write(\"musicgen_out.wav\", music[\"audio\"][0].T, music[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c3025-4e78-4cd0-9f68-d0ecc1315abc",
   "metadata": {},
   "source": [
    "- 代码说明：\n",
    "    - 音频生成的结果包含\"audio\"与\"sampling_rate\"字段，也是保存为音频文件必须的两个参数。\n",
    "    - 注意音频数据是numpy.ndarray，但是生成的形状是(2, n)，所以需要转置下，形状转换为（n，2）,2表示立体声。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c429699-33c3-40ce-82b8-b69b0f4803a6",
   "metadata": {},
   "source": [
    "### (6) 使用Ace-Step1.5模型生成音乐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407600a-3e5b-4ffe-93e9-4870362e9ef8",
   "metadata": {},
   "source": [
    "- Ace-Step1.5模型实际是由四个模型协作完成音乐生成的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326beae3-0a9d-4b3f-961f-3f0b03694a56",
   "metadata": {},
   "source": [
    "- Ace-Step1.5模型结构\n",
    "    - `Qwen3-Embedding-0.6B`：语义理解层(Semantic Understanding)\n",
    "        - 音乐生成的第一步不是生成声音，而是理解意图。当你输入“赛博朋克风格的重低音，像银翼杀手那样”时，这个模型将这句话转化为一个 768维或1024维的数学向量 (Embeddings)。\n",
    "    - `acestep-5Hz-lm-1.7B`：全局规划层 (Global Planning)\n",
    "        - 它接收Qwen传来的向量，然后通过自回归(Autoregressive)的方式，预测出一串语义Token(Semantic Tokens)。\n",
    "        - 5Hz表示每秒只生成5个Token。\n",
    "    - `acestep-v15-turbo`：音频合成层 (Acoustic Synthesis)\n",
    "        - 这是DiT(Diffusion Transformer)模型。\n",
    "        - turbo后缀意味着：使用了对抗动态位移蒸馏 (Adversarial Dynamic-Shift Distillation)技术训练而来。\n",
    "        - 它拿着上一步LM给出的“粗糙骨架”，利用扩散算法，通过去噪过程，将其还原成细腻的 Mel-Spectrogram (梅尔频谱图)。\n",
    "    - `vae (Variational Autoencoder)`：信号解码层 (Signal Decoding)\n",
    "        - 计算机内部处理的是频谱图（一种类似热力图的图像），VAE的作用就是将频谱图无损地转换成wav音频波形格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58611d3-1614-4cfe-bdd9-cea1dbb8023c",
   "metadata": {},
   "source": [
    "- Ace-Step1.5模型的创新突破\n",
    "    - 传统的音乐模型（如 MusicGen）试图用一个Transformer模型生成音乐：既生成音乐的结构（主歌/副歌），也生成音质（乐器纹理）。结果往往是：生成音乐超过一定时间（比如1分钟），生成的音乐结构与乐器声音质量变差（比如乐器声音变得模糊）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f46cc36-108a-439a-8166-c2542ee8028b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载核心模型：acestep-v15-turbo\n",
      "加载音频解码模型：vae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python313\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载意图理解模型：Qwen3-Embedding-0.6B【分词器与模型】\n",
      "音频生成的音乐节奏意图理解\n",
      "音频生成的歌词理解\n",
      "加载参考音频\n",
      "音乐生成......\n",
      "音频解码\n",
      "保存音频文件为：AceStep_out.wav\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers.generation.streamers import BaseStreamer\n",
    "from diffusers.models import AutoencoderOobleck\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# ===============加载模型=========================\n",
    "# 1. 加载核心模型：生成音乐编码的模型\n",
    "print(\"加载核心模型：acestep-v15-turbo\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"F:/03Models/Ace-Step1.5/acestep-v15-turbo\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",   \n",
    "    dtype=torch.bfloat16  # flash_attention_2只支持bfloat16与float16.\n",
    ")\n",
    "\n",
    "model.config._attn_implementation = \"flash_attention_2\"   # 受用flash_attention_2算法\n",
    "model = model.to(\"cuda:0\").to(torch.bfloat16)\n",
    "model.eval()   # 推理模式\n",
    "\n",
    "# 2. 音频解码模型，把上面音频模型的输出，解码成音频格式，\n",
    "print(\"加载音频解码模型：vae\")\n",
    "vae = AutoencoderOobleck.from_pretrained(\n",
    "    \"F:/03Models/Ace-Step1.5/vae\"\n",
    ")\n",
    "vae = vae.to(\"cuda:0\").to(torch.bfloat16)\n",
    "vae.eval()\n",
    "\n",
    "# 3. 理解用户意图的语言大模型\n",
    "print(\"加载意图理解模型：Qwen3-Embedding-0.6B【分词器与模型】\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(     \n",
    "    \"F:/03Models/Ace-Step1.5/Qwen3-Embedding-0.6B\"\n",
    ")   # 分词器\n",
    "text_encoder = AutoModel.from_pretrained(\n",
    "    \"F:/03Models/Ace-Step1.5/Qwen3-Embedding-0.6B\"\n",
    ")  # 意图输出。输出的张量被核心模型，用来生成音频。\n",
    "\n",
    "text_encoder = text_encoder.to(\"cuda:0\").to(torch.bfloat16)\n",
    "text_encoder.eval()\n",
    "\n",
    "\n",
    "#///////////////音乐生成参数与意图理解///////////////////////////////\n",
    "# 1. 音乐生成的参数\n",
    "# caption=\"欢快的电子舞曲，重低音\"\n",
    "caption=\"悠扬中国古筝民乐\"\n",
    "lyrics=\"\"   # 歌词\n",
    "bpm=128     # 控制音乐节奏快慢（Beats Per Minute，每分钟节拍数）\n",
    "duration=30   # 生成音乐的时长\n",
    "batch_size=1  # 可以生成多个音乐，这里我们设置为一个。\n",
    "# audio_format=\"flac\"   # 音频文件格式，这个本来是使用torchaudio来保存需要使用的，但本代码中还是使用soundfile框架保存的。\n",
    "# save_dir=\"./\"         # 音频保存路径，我们直接使用当前路径，torchaudio只需要指定路径，音频文件会随机生成。\n",
    "instruction = \"Fill the audio semantic mask based on the given conditions:\"  # 这是生成输入需要指定的音乐结构\n",
    "language = \"unknown\"  # 生成歌词的语言，我们这里没有生成歌词，所以使用了unknown\n",
    "sample_rate = 48000   # 音频文件的波特率。\n",
    "\n",
    "# 2. 音乐意图理解的参数\n",
    "text_prompt = F\"\"\"# Instruction\n",
    "{instruction}\n",
    "\n",
    "# Caption\n",
    "{caption}\n",
    "\n",
    "# Metas\n",
    "- bpm: {bpm}\n",
    "- timesignature: N/A\n",
    "- keyscale: N/A\n",
    "- duration: {duration} seconds\n",
    "<|endoftext|>\n",
    "\"\"\"\n",
    "print(\"音频生成的音乐节奏意图理解\")\n",
    "text_inputs_dict = text_tokenizer(\n",
    "    text_prompt,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "text_hidden_states = text_inputs_dict[\"input_ids\"].to(\"cuda\")\n",
    "text_attention_mask = text_inputs_dict[\"attention_mask\"].to(\"cuda\")\n",
    "text_hidden_states = text_encoder(input_ids=text_hidden_states, lyric_attention_mask=None).last_hidden_state\n",
    "\n",
    "# 3. 歌词的理解\n",
    "print(\"音频生成的歌词理解\")\n",
    "lyrics_text = f\"# Languages\\n{language}\\n\\n# Lyric\\n{lyrics}<|endoftext|>\"\n",
    "lyrics_inputs_dict = text_tokenizer(\n",
    "    lyrics_text,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=2048,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "lyric_hidden_states = lyrics_inputs_dict[\"input_ids\"].to(\"cuda\")\n",
    "lyric_attention_mask = lyrics_inputs_dict[\"attention_mask\"].to(\"cuda\")\n",
    "# lyric_hidden_states = text_encoder(input_ids=lyric_hidden_states, lyric_attention_mask=None).last_hidden_state\n",
    "lyric_hidden_states = text_encoder.embed_tokens(lyric_hidden_states)\n",
    "\n",
    "# 参考音频\n",
    "print(\"加载参考音频\")\n",
    "# refer_audios = [[torch.zeros(2, 30 * sample_rate)] for _ in range(batch_size)]\n",
    "refer_audio_latents = torch.load(\"F:/03Models/Ace-Step1.5/acestep-v15-turbo/silence_latent.pt\", weights_only=True).transpose(1, 2)\n",
    "refer_audio_latents = refer_audio_latents.to(\"cuda\").to(torch.bfloat16)\n",
    "expected_latent_length = duration * sample_rate // 1920   # 1920会影响音频时间长度(而且vea解码内存可能不足够)\n",
    "# expected_latent_length = duration * sample_rate // 480   # 1920会影响音频时间长度\n",
    "refer_audio_latents = refer_audio_latents[:, :expected_latent_length, :]\n",
    "refer_audio_order_mask = torch.tensor([0], device=\"cuda\", dtype=torch.long)    # [0]个数与batch_size一致\n",
    "\n",
    "chunk_mask = torch.ones(expected_latent_length, dtype=torch.bool, device=\"cuda\")\n",
    "chunk_mask = chunk_mask.unsqueeze(-1).repeat(1, 1, refer_audio_latents.shape[2])\n",
    "src_latents = refer_audio_latents.clone()\n",
    "is_covers=torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "\n",
    "#///////////////////生成音频////////////////////////\n",
    "print(\"音乐生成......\")\n",
    "outputs = model.generate_audio(\n",
    "    text_hidden_states=text_hidden_states,\n",
    "    text_attention_mask=text_attention_mask,\n",
    "    lyric_hidden_states=lyric_hidden_states,\n",
    "    lyric_attention_mask=lyric_attention_mask,\n",
    "    refer_audio_acoustic_hidden_states_packed=refer_audio_latents,\n",
    "    refer_audio_order_mask=refer_audio_order_mask,\n",
    "    src_latents=src_latents,\n",
    "    chunk_masks=chunk_mask,\n",
    "    is_covers=is_covers\n",
    ")\n",
    "#///////////////////音频解码并保存////////////////////////\n",
    "# 1. 音频解码\n",
    "print(\"音频解码\")\n",
    "pred_latents = outputs[\"target_latents\"]\n",
    "pred_latents_cpu = pred_latents.clone()\n",
    "pred_latents_for_decode = pred_latents.transpose(1, 2).contiguous()\n",
    "pred_latents_for_decode = pred_latents_for_decode.to(vae.dtype)\n",
    "\n",
    "decoder_output = vae.decode(pred_latents_for_decode)\n",
    "pred_wavs = decoder_output.sample\n",
    "pred_wavs = decoder_output.sample.detach().to(\"cpu\").float().numpy()\n",
    "# 2. 保存为wav文件\n",
    "print(\"保存音频文件为：AceStep_out.wav\")\n",
    "sf.write(\"AceStep_out.wav\", pred_wavs[0].T, samplerate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be63073-4e34-4641-94b2-4ac672b23ae6",
   "metadata": {},
   "source": [
    "- 代码说明:\n",
    "    - 音乐生成最核心的模型是三个：\n",
    "        - 音乐与歌词意图理解模型：Qwen3-Embedding-0.6B\n",
    "        - 音乐生成模型：acestep-v15-turbo\n",
    "        - 音频解码模型：vae\n",
    "        - 注意：在Ace-Step1.5模型中，还有一个音乐与歌词细节补充模型：acestep-5Hz-lm-1.7B\n",
    "    - 在vae解码的时候，为了节省内存，可以采用分块解码的方法，我们是直接一次性解码，在本人机器上，大约也就只能生成30-40秒的音频。\n",
    "    - 本代码仅仅是演示Ace-Step1.5模型的工作步骤与流程，其实还有很多细节可以强化处理。Ace-Step1.5模型还可以完成如下工作：\n",
    "        - 更加长的音乐生成（acestep-5Hz-lm-1.7B模型：全能规划器模型）。\n",
    "        - 歌词生成。\n",
    "        - 翻唱生成。\n",
    "        - 重绘（repainting）。\n",
    "        - 人声转背景音乐（BGM）。\n",
    "        - 支持50多种语言的提示词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4a0dd-305e-488c-b75b-77c4b74ab2d1",
   "metadata": {},
   "source": [
    "## 4.2. 文本转语音任务的技术分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb149e99-d83b-4333-a8a3-14374c2a265b",
   "metadata": {},
   "source": [
    "- 在技术分析中还是重点关注输入数据的处理，以及输出数据的处理。并能了解模型的网络结构与核心特点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79d711-a4c5-48b5-a17e-8c914bb9385d",
   "metadata": {},
   "source": [
    "- 明显语音模型有很多，重点掌握几个经典的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eaefd5-f3cb-435e-a300-b8ac875f2651",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122025be-eae5-49af-84f4-e489673b775f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
