{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a48391a-df40-465d-881e-e45e925c4786",
   "metadata": {},
   "source": [
    "# 7. 图像转文本任务应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b7820-3cdd-4247-8c1e-cd9f1e69f1c2",
   "metadata": {},
   "source": [
    "## 7.1. 图像描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534ef5d-1ccb-45fe-a52c-090f0385c0fa",
   "metadata": {},
   "source": [
    "- 图像描述生成是指为图像生成文本描述的过程（看图说话）。这可以帮助视障人士理解周围环境的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70e486-7be0-4a23-808d-116f17d7e3cb",
   "metadata": {},
   "source": [
    "### (1) 模型下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915ee84-b1a3-4f4c-ae26-6bd9b8622dec",
   "metadata": {},
   "source": [
    "- 下载指令：\n",
    "    - `git clone https://www.modelscope.cn/Salesforce/blip-image-captioning-large.git`\n",
    "    - `git clone https://www.modelscope.cn/Salesforce/blip-image-captioning-base.git`\n",
    "    - `git clone https://www.modelscope.cn/microsoft/git-base.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2c9e8-fa64-45c0-8a78-ce213a4f3894",
   "metadata": {},
   "source": [
    "### (2) 模型任务应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f1a291-1660-4a45-89bb-ff994ebac7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.pipelines.image_to_text.ImageToTextPipeline'>\n",
      "<class 'transformers.models.blip.modeling_blip.BlipForConditionalGeneration'>\n",
      "<class 'transformers.models.blip.configuration_blip.BlipConfig'>\n",
      "<class 'NoneType'>\n",
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.blip.image_processing_blip.BlipImageProcessor'>\n",
      "[{'generated_text': 'there are two parrots that are standing next to each other'}]\n",
      "[{'generated_text': 'people are walking and riding bikes on a busy street'}]\n",
      "[{'generated_text': 'there is a large cloud that is in the sky over a field'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "captioner = pipeline(\"image-to-text\", model=\"F:/03Models/blip-image-captioning-large\", use_fast=True)\n",
    "print(type(captioner))\n",
    "print(type(captioner.model))\n",
    "print(type(captioner.model.config))\n",
    "print(type(captioner.tokenizer))\n",
    "print(type(captioner.image_processor))\n",
    "print(captioner(\"./imgs/parrots.png\"))\n",
    "print(captioner(\"./imgs/street.jpg\"))\n",
    "print(captioner(\"./imgs/scene.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ff7b43-1313-4421-a95f-60cd8d49b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.pipelines.image_to_text.ImageToTextPipeline'>\n",
      "<class 'transformers.models.git.modeling_git.GitForCausalLM'>\n",
      "<class 'transformers.models.git.configuration_git.GitConfig'>\n",
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.clip.image_processing_clip.CLIPImageProcessor'>\n",
      "[{'generated_text': 'the parrots are looking at each other'}]\n",
      "[{'generated_text': 'a street light on a pole'}]\n",
      "[{'generated_text': 'a large cloud in the sky'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "captioner = pipeline(\"image-to-text\", model=\"F:/03Models/git-base\", use_fast=True)\n",
    "print(type(captioner))\n",
    "print(type(captioner.model))\n",
    "print(type(captioner.model.config))\n",
    "print(type(captioner.tokenizer))\n",
    "print(type(captioner.image_processor))\n",
    "print(captioner(\"./imgs/parrots.png\"))\n",
    "print(captioner(\"./imgs/street.jpg\"))\n",
    "print(captioner(\"./imgs/scene.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f788d-41e4-45ce-9809-1f2af0b2997e",
   "metadata": {},
   "source": [
    "## 7.2. 光学符号识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4d01d-e246-4014-a3da-c7e071b3dcc0",
   "metadata": {},
   "source": [
    "- `git clone https://www.modelscope.cn/allenai/olmOCR-7B-0725.git`\n",
    "- `git clone https://huggingface.co/zai-org/GLM-OCR`\n",
    "- `git clone https://www.modelscope.cn/microsoft/trocr-base-handwritten.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e118cbae-473e-43d7-8379-5c5923c539e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python313\\Lib\\site-packages\\PIL\\Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at F:/03Models/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '4.8.9 .'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at F:/03Models/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.9 .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "img_file = './imgs/hand.png'\n",
    "image = Image.open(img_file).convert(\"RGB\")\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"image-to-text\", model=\"F:/03Models/trocr-large-handwritten\")\n",
    "print(pipe(image))\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('F:/03Models/trocr-large-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('F:/03Models/trocr-large-handwritten')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbfaa9-bd51-44de-9cd2-2080382b7086",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ce1ed2-85f7-42af-b7fc-2df86af5d299",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
